---
description: >-
  Before you start this page, please confirm which or whether Workload Manager
  for the lab server is used at UA. Slurm is used in BMB lab 610 at HZAU.
---

# 3.4 Slurm

#### 3.4.1 Introduction

**Slurm** \(Simple Linux Utility Resource Management\) ****is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters.

**Partitions** are logical collections of nodes that comprise different hardware resources and limits to help meet the wide variety of jobs that get scheduled on Discovery. Occasionally, the Research Computing team might need to make updates to the partitions based on monitoring job submissions to help reduce job wait times.

The`sbatch` **command** is a bash script used to specify the resources you need to run your jobs, such as the number of nodes you want to run your job. Then, Slurm schedules your job based on the availability of the resources you’ve specified.

#### 3.4.2 Get familiar with the lab server

Run `shosts`.

```text
(base) [nanzhen@admin ~]$ shosts
>>> big             node1           66/66           load average: 66.66
>>> pig             node2           55/66         load average: 6.66
>>> normal          node3           0/66            load average: 0.01

```

#### 3.4.3 Test SBATCH 

**Step 1.** Run `vim practice_sbatch.sh`.

**Step 2.** Type the following content.

```text
#!/bin/sh
#SBATCH -J nanzhen_virsorter2
#SBATCH -c 30
#SBATCH -p normal
```

`/sh`, or `/bash`

`-J`, or`--job-name=jobname`. In this example, a job named nanzhen\_virsorter2 is submitted.

`-c`, or `--cpus-per-task=ncpus`, number of CPUs required per task

`-p`, or`--partition=partition`. In this example, the chosen partition name is normal.

{% hint style="info" %}
Try to run `sbatch --help`, `sinfo`, `sinfo -p <partition name>`, `sinfo --partition <partition name>`, etc.

`-N`, `--nodes=N`,  number of nodes on which to run. Normally, one node is chosen.
{% endhint %}

**Step 3.** Run `sbatch practice_sbatch.sh`.

**Step4.** waiting......learning....

**Step5.** Run `squeue` to see all submitted jobs.

#### 3.4.4 Slurm cheatsheet

{% file src="../../.gitbook/assets/slurm\_cheetsheet.pdf" caption="Slurm\_cheetsheet" %}

#### 3.4.5 References

* https://slurm.schedmd.com/overview.html
* https://rc-docs.northeastern.edu/en/latest/hardware/partitions.html\#viewing-partition-information
* https://www.cuhk.edu.hk/itsc/hpc/slurm.html
* http://hpc.pku.edu.cn/\_book/guide/slurm/sbatch.html ⭐️ ⭐️⭐️



